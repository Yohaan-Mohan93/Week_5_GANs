{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30775,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport pathlib\nimport time\nimport datetime\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom matplotlib import pyplot as plt\nfrom IPython import display\nimport PIL\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nTRAIN_FILES = []\nTEST_FILES = []\nfor dirname, _, filenames in os.walk('../input/gan-getting-started/monet_tfrec'):\n    for filename in filenames:\n         TRAIN_FILES.append(os.path.join(dirname, filename))\n            \nfor dirname, _, filenames in os.walk('../input/gan-getting-started/photo_tfrec'):\n    for filename in filenames:\n         TEST_FILES.append(os.path.join(dirname, filename))\n\nTRAIN_FILES","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-01T23:49:52.961387Z","iopub.execute_input":"2024-10-01T23:49:52.961870Z","iopub.status.idle":"2024-10-01T23:49:57.696872Z","shell.execute_reply.started":"2024-10-01T23:49:52.961825Z","shell.execute_reply":"2024-10-01T23:49:57.695611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. **Data Understanding and Augmentation:**\n* Goal: Enhance the dataset to create more robust training data.\n* Action: Use the jittering, cropping, and normalization steps you've already implemented. Consider additional augmentation methods like rotation, brightness adjustment, and contrast changes to further diversify the dataset.\n2. **Model Selection:**\n* Goal: Choose appropriate deep learning architectures that align with your task (e.g., image translation, classification).\n* Options:\n    * Generative Adversarial Networks (GANs) for style transfer if you're working with Monet paintings vs. photos.\n    * Convolutional Neural Networks (CNNs) for classification tasks.\n* Transfer Learning using pre-trained models like VGG, ResNet, or Inception if the dataset size is limited.\n* Action: Start with simpler models and incrementally move to more complex ones as needed.\n3. **Training the Model:**\n* Goal: Optimize model training by tuning hyperparameters such as learning rate, batch size, and number of epochs.\n* Action:\n    * Split the dataset into training, validation, and test sets.\n    * Apply the pre-processing pipeline to all datasets to ensure consistency.\n    * Use callbacks like early stopping to prevent overfitting.\n4. **Evaluation Metrics:**\n* Goal: Measure the performance of your model effectively.\n* Metrics:\n    * For image generation: Inception Score, Fréchet Inception Distance (FID).\n    * For classification: Accuracy, Precision, Recall, F1-score.\n    * Visual evaluation: Compare generated vs. real images side-by-side.\n* Action: Implement metric-specific evaluation functions and generate visual comparisons to assess qualitative performance.\n5. **Error Analysis:**\n* Goal: Identify areas where the model performs poorly and understand why.\n* Action:\n    * Review misclassified images or poorly generated samples.\n    * Check for biases in training data or misalignments during pre-processing.\n    * Use techniques like Grad-CAM for CNNs to interpret what the model is focusing on.\n6. **Model Fine-tuning and Optimization:**\n* Goal: Refine the model for better performance.\n* Action:\n    * Perform hyperparameter tuning using grid search or random search.\n    * Experiment with different layers or architectures.\n    * Consider ensemble techniques if using classification models.","metadata":{}},{"cell_type":"markdown","source":"# Step 1: Data Understanding, Introduction and Augementation\n\n**Problem Description:**<br>\nThe \"I'm Something of a Painter Myself\" Kaggle challenge focuses on generating Monet-style artworks using generative deep learning models, specifically GANs (Generative Adversarial Networks). The main goal is to train a model that can transform regular photos into paintings that mimic Monet's distinctive style. This task combines artistic style transfer with deep learning, pushing participants to explore advanced generative modeling techniques.\n\n**Generative Deep Learning Models:**<br>\nGenerative deep learning models, such as GANs, consist of two main components: a generator and a discriminator. The generator creates new data instances (in this case, Monet-style images), while the discriminator evaluates them against real Monet paintings to distinguish between authentic and generated artworks. The iterative competition between these two models helps improve the quality of the generated images.\n\n**Dataset Overview:**<br>\nThe dataset contains four directories:\n* Monet Images: Includes 300 Monet paintings sized 256x256 pixels available in both JPEG and TFRecord formats.\n* Photo Images: Contains 7,028 photos sized 256x256 pixels, also provided in JPEG and TFRecord formats.","metadata":{}},{"cell_type":"code","source":"# Define function to parse TFRecord images\ndef parse_tfrecord_fn(example):\n    feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'image_name': tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(example, feature_description)\n    image = tf.image.decode_jpeg(example['image'], channels=3)\n    image = tf.image.resize(image, [256, 256])\n    return image\n\n# Load the Monet and photo datasets\ndef load_dataset(filenames):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False  # for better performance\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.with_options(ignore_order)\n    dataset = dataset.map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE)\n    return dataset\n\nmonet_ds = load_dataset(TRAIN_FILES)\nphoto_ds = load_dataset(TEST_FILES)\n\n# Function to display images from a dataset\ndef display_samples(dataset, n_samples):\n    plt.figure(figsize=(12, 12))\n    for i, image in enumerate(dataset.take(n_samples)):\n        plt.subplot(1, n_samples, i + 1)\n        plt.imshow(image.numpy().astype(\"uint8\"))\n        plt.axis(\"off\")\n    plt.show()\n\n# Display some Monet paintings\ndisplay_samples(monet_ds, 5)\n\n# Display some photos\ndisplay_samples(photo_ds, 5)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:49:57.698419Z","iopub.execute_input":"2024-10-01T23:49:57.699132Z","iopub.status.idle":"2024-10-01T23:49:58.680975Z","shell.execute_reply.started":"2024-10-01T23:49:57.699085Z","shell.execute_reply":"2024-10-01T23:49:58.679719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_image_stats(dataset):\n    pixel_values = []\n    for image in dataset:\n        pixel_values.append(tf.reshape(image, [-1]))  # Flatten image to 1D\n    pixel_values = tf.concat(pixel_values, axis=0)\n    mean = tf.reduce_mean(pixel_values).numpy()\n    stddev = tf.math.reduce_std(pixel_values).numpy()\n    return mean, stddev\n\n# Compute stats for Monet paintings\nmonet_mean, monet_stddev = compute_image_stats(monet_ds)\nprint(f\"Monet Paintings - Mean pixel value: {monet_mean}, StdDev: {monet_stddev}\")\n\n# Compute stats for photos\nphoto_mean, photo_stddev = compute_image_stats(photo_ds)\nprint(f\"Photos - Mean pixel value: {photo_mean}, StdDev: {photo_stddev}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:49:58.682958Z","iopub.execute_input":"2024-10-01T23:49:58.683567Z","iopub.status.idle":"2024-10-01T23:50:18.727619Z","shell.execute_reply.started":"2024-10-01T23:49:58.683507Z","shell.execute_reply":"2024-10-01T23:50:18.726385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n\nSetting up the datasets","metadata":{}},{"cell_type":"code","source":"raw_monet_ds = tf.data.TFRecordDataset(TRAIN_FILES)\nraw_painting_ds = tf.data.TFRecordDataset(TEST_FILES)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:50:18.731416Z","iopub.execute_input":"2024-10-01T23:50:18.731914Z","iopub.status.idle":"2024-10-01T23:50:18.765954Z","shell.execute_reply.started":"2024-10-01T23:50:18.731871Z","shell.execute_reply":"2024-10-01T23:50:18.764678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for raw_record in raw_monet_ds.take(1):\n    example = tf.train.Example()\n    example.ParseFromString(raw_record.numpy())\n\n    result = {}\n\n# example.features.feature is the dictionary\nfor key, feature in example.features.feature.items():\n    # The values are the Feature objects which contain a `kind` which contains:\n    # one of three fields: bytes_list, float_list, int64_list\n    kind = feature.WhichOneof('kind')\n    result[key] = np.array(getattr(feature, kind).value).dtype","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:50:18.767648Z","iopub.execute_input":"2024-10-01T23:50:18.768050Z","iopub.status.idle":"2024-10-01T23:50:18.795802Z","shell.execute_reply.started":"2024-10-01T23:50:18.768009Z","shell.execute_reply":"2024-10-01T23:50:18.794686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a description of the features.\nfeature_description = {\n    'target': tf.io.FixedLenFeature([], tf.string, default_value=''),\n    'image_name': tf.io.FixedLenFeature([], tf.string, default_value=''),\n    'image': tf.io.FixedLenFeature([], tf.string, default_value=''),\n}\n\ndef _parse_image(example_proto):\n  # Parse the input tf.train.Example proto using the dictionary above.\n  return tf.io.parse_single_example(example_proto, feature_description)\n\nmonet_train_ds = raw_monet_ds.map(_parse_image)\npainting_test_ds = raw_painting_ds.map(_parse_image)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:50:18.797199Z","iopub.execute_input":"2024-10-01T23:50:18.797632Z","iopub.status.idle":"2024-10-01T23:50:18.889026Z","shell.execute_reply.started":"2024-10-01T23:50:18.797587Z","shell.execute_reply":"2024-10-01T23:50:18.887795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Each image is 256x256 in size\nIMAGE_SIZE = 256\nCHANNELS = 3\nEPOCHS = 10\n\ndef random_crop(image):\n    cropped_image = tf.image.random_crop(image, size=[IMAGE_SIZE, IMAGE_SIZE, CHANNELS])\n\n    return cropped_image\n\n# normalizing the images to [-1, 1]\ndef normalize(image):\n    image = tf.cast(image, tf.float32)\n    image = (image / 127.5) - 1\n    return image\n\ndef random_jitter(image):\n    # resizing to 286 x 286 x 3\n    image = tf.image.resize(image, [286, 286], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n    # randomly cropping to 256 x 256 x 3\n    image = random_crop(image)\n\n    # random mirroring\n    image = tf.image.random_flip_left_right(image)\n\n    return image\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:50:18.890635Z","iopub.execute_input":"2024-10-01T23:50:18.891030Z","iopub.status.idle":"2024-10-01T23:50:18.900329Z","shell.execute_reply.started":"2024-10-01T23:50:18.890988Z","shell.execute_reply":"2024-10-01T23:50:18.899005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The facade training set consist of 300 images\nBUFFER_SIZE = 300\nBATCH_SIZE = 30\n\ndef preprocess_image_train(batch):\n    image = batch['image']\n    image = tf.io.decode_jpeg(image, channels=CHANNELS)\n    image = random_jitter(image)\n    image = normalize(image)\n    return image\n\ndef preprocess_image_test(batch):\n    image = batch['image']\n    image = tf.io.decode_jpeg(image, channels=CHANNELS)\n    image = normalize(image)\n    return image\n\ndef create_dataset_train(dataset):\n    ds = dataset.map(lambda batch: preprocess_image_train(batch), num_parallel_calls=AUTOTUNE)\n    ds = ds.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n    return ds.prefetch(AUTOTUNE)\n\ndef create_dataset_test(dataset):\n    ds = dataset.map(lambda batch: preprocess_image_test(batch), num_parallel_calls=AUTOTUNE)\n    ds = ds.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n    return ds.prefetch(AUTOTUNE)\n\ntrain_ds = create_dataset_train(monet_train_ds)\ntest_ds = create_dataset_test(painting_test_ds)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:50:18.902114Z","iopub.execute_input":"2024-10-01T23:50:18.902609Z","iopub.status.idle":"2024-10-01T23:50:19.299593Z","shell.execute_reply.started":"2024-10-01T23:50:18.902543Z","shell.execute_reply":"2024-10-01T23:50:19.298251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Model Selection\nFor model selection in the context of our current project, we need to choose models that are well-suited for handling images, especially considering the extensive augmentation and preprocessing pipeline you've developed. Here are the key considerations and potential models that align with your task:\n\n1. Convolutional Neural Networks (CNNs)\n* Why Use CNNs? CNNs are highly effective for image classification and generation tasks due to their ability to capture spatial hierarchies and patterns within images.\n* Popular Architectures:\n    * ResNet (Residual Networks): Known for handling deep models with residual connections, making them less prone to vanishing gradients.\n    * VGGNet: Provides simplicity and effectiveness with stacked convolutional layers, though it has a higher number of parameters.\n    * Inception (GoogLeNet): Efficient with depthwise separable convolutions and multiple filter sizes that capture varying features simultaneously.\n    * MobileNet: Lightweight architecture suitable for tasks requiring efficiency, especially useful if training on limited computational resources.\n2. Transfer Learning Models\n* Why Use Transfer Learning? <br>\n    * Leveraging pre-trained models, such as those trained on large datasets like ImageNet, can significantly speed up training and improve performance with limited data. You can fine-tune these models with your data to enhance their capability for specific tasks.\n* Models to Consider:\n    * ResNet50: Effective for image classification, suitable for transfer learning with relatively few parameters compared to other deep networks.\n    * EfficientNet: Provides a good trade-off between model size and accuracy, scaling width, depth, and resolution systematically.\n    * DenseNet: Uses dense connections between layers to improve feature reuse, leading to better parameter efficiency.\n3. Generative Adversarial Networks (GANs)\n* Why Use GANs? \n    * If your task involves image generation or style transfer, GANs can be incredibly powerful. They work by training two neural networks (generator and discriminator) in a game-theoretic setting, leading to realistic image outputs.\n* Models to Consider:\n    * Pix2Pix: For paired image-to-image translation tasks, where input-output pairs are available.\n    * CycleGAN: For unpaired image-to-image translation, allowing transformation between two domains without paired examples.\n4. Vision Transformers (ViTs)\n* Why Use ViTs? \n    * Vision Transformers have recently gained popularity due to their ability to capture long-range dependencies and are state-of-the-art in many image classification tasks. They split images into patches and process them similarly to how transformers handle sequences in NLP.\n* Models to Consider:\n    * ViT-Base: The base version of the Vision Transformer, good for classification tasks.\n    * DeiT (Data-efficient Image Transformers): Designed to be more efficient and effective when trained with less data compared to traditional ViTs.\n5. Model Evaluation Metrics\n* Accuracy: Basic metric for classification tasks, though not always the best if class imbalances exist.\n* Precision, Recall, and F1-score: Useful for tasks where the balance between false positives and false negatives matters.\n* Mean Absolute Error (MAE) and Mean Squared Error (MSE): Common for regression-based evaluations.\n* Inception Score (IS) and Fréchet Inception Distance (FID): Specific to evaluating the quality of generated images from GANs.","metadata":{}},{"cell_type":"markdown","source":"**CycleGAN**\n\nA CycleGAN involves implementing two main components: a generator and a discriminator for each domain (e.g., Monet paintings and photos). The key difference with CycleGAN is its ability to translate images between two domains without paired data using two sets of generators and discriminators","metadata":{}},{"cell_type":"code","source":"#Build the downsampler (encoder)¶\n#Structure: Convolution -> Instance normalization -> Leaky ReLU\ndef downsample(filters, size, apply_norm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = models.Sequential()\n    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n                             kernel_initializer=initializer, use_bias=False))\n    if apply_norm:\n        result.add(layers.GroupNormalization(groups=-1))\n    result.add(layers.LeakyReLU())\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:50:19.301264Z","iopub.execute_input":"2024-10-01T23:50:19.302450Z","iopub.status.idle":"2024-10-01T23:50:19.309974Z","shell.execute_reply.started":"2024-10-01T23:50:19.302364Z","shell.execute_reply":"2024-10-01T23:50:19.308573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Build the upsampler(decoder)\n#Structure: Transposed convolution -> Instance normalization -> Dropout (applied to the first 3 blocks) -> ReLU\ndef upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = models.Sequential()\n    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n                                      kernel_initializer=initializer, use_bias=False))\n    result.add(layers.GroupNormalization(groups=-1))\n    if apply_dropout:\n        result.add(layers.Dropout(0.5))\n    result.add(layers.ReLU())\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:50:19.311603Z","iopub.execute_input":"2024-10-01T23:50:19.312014Z","iopub.status.idle":"2024-10-01T23:50:19.323650Z","shell.execute_reply.started":"2024-10-01T23:50:19.311972Z","shell.execute_reply":"2024-10-01T23:50:19.322433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Build the generator with the encoder and decoder\ndef make_generator():\n    inputs = layers.Input(shape=[IMAGE_SIZE, IMAGE_SIZE, CHANNELS])\n\n    down_stack = [\n        downsample(64, 4, apply_norm=False),\n        downsample(128, 4),\n        downsample(256, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n    ]\n    \n    up_stack = [\n        upsample(512, 4, apply_dropout=True),\n        upsample(512, 4, apply_dropout=True),\n        upsample(512, 4, apply_dropout=True),\n        upsample(512, 4),\n        upsample(256, 4),\n        upsample(128, 4),\n        upsample(64, 4),\n    ]\n    \n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = layers.Conv2DTranspose(3, 4, strides=2, padding='same',\n                                  kernel_initializer=initializer, activation='tanh')\n    \n    x = inputs\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n    \n    skips = reversed(skips[:-1])\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = layers.Concatenate()([x, skip])\n    \n    x = last(x)\n    return models.Model(inputs=inputs, outputs=x)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:50:19.325464Z","iopub.execute_input":"2024-10-01T23:50:19.325917Z","iopub.status.idle":"2024-10-01T23:50:19.339875Z","shell.execute_reply.started":"2024-10-01T23:50:19.325856Z","shell.execute_reply":"2024-10-01T23:50:19.338779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Build the discriminator based on a CNN-based image classifier:\ndef make_discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    inputs = layers.Input(shape=[IMAGE_SIZE, IMAGE_SIZE, 3], name='input_image')\n    \n    x = inputs\n    x = downsample(64, 4, False)(x)\n    x = downsample(128, 4)(x)\n    x = downsample(256, 4)(x)\n    x = layers.ZeroPadding2D()(x)\n    x = layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(x)\n    x = layers.GroupNormalization(groups=-1)(x)\n    x = layers.LeakyReLU()(x)\n    x = layers.ZeroPadding2D()(x)\n    x = layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(x)\n    return models.Model(inputs=inputs, outputs=x)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:50:19.341160Z","iopub.execute_input":"2024-10-01T23:50:19.341603Z","iopub.status.idle":"2024-10-01T23:50:19.352463Z","shell.execute_reply.started":"2024-10-01T23:50:19.341558Z","shell.execute_reply":"2024-10-01T23:50:19.350891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the requried generators and discriminators\nmonet_generator = make_generator()\nphoto_generator = make_generator()\nmonet_discriminator = make_discriminator()\nphoto_discriminator = make_discriminator()","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:50:19.357698Z","iopub.execute_input":"2024-10-01T23:50:19.358163Z","iopub.status.idle":"2024-10-01T23:50:21.363500Z","shell.execute_reply.started":"2024-10-01T23:50:19.358116Z","shell.execute_reply":"2024-10-01T23:50:21.362228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the loss function functions \ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss * 0.5\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n\nLAMBDA = 10\n\ndef compute_cycle_loss(real_img, cycled_img):\n    loss = tf.reduce_mean(tf.abs(real_img - cycled_img))\n    return LAMBDA * loss\n\ndef identity_loss(real_img, same_img):\n    loss = tf.reduce_mean(tf.abs(real_img - same_img))\n    return LAMBDA * 0.5 * loss","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:50:21.364936Z","iopub.execute_input":"2024-10-01T23:50:21.365295Z","iopub.status.idle":"2024-10-01T23:50:21.374765Z","shell.execute_reply.started":"2024-10-01T23:50:21.365257Z","shell.execute_reply":"2024-10-01T23:50:21.373484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the optimizers for all the generators and the discriminator\nmonet_generator_optimizer = Adam(2e-4, beta_1=0.5)\nphoto_generator_optimizer = Adam(2e-4, beta_1=0.5)\nmonet_discriminator_optimizer = Adam(2e-4, beta_1=0.5)\nphoto_discriminator_optimizer = Adam(2e-4, beta_1=0.5)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:50:21.376173Z","iopub.execute_input":"2024-10-01T23:50:21.376600Z","iopub.status.idle":"2024-10-01T23:50:21.400359Z","shell.execute_reply.started":"2024-10-01T23:50:21.376555Z","shell.execute_reply":"2024-10-01T23:50:21.399183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a check\nCHECKPOINT_DIR = '../working/checkpoints/train'\n\nckpt = tf.train.Checkpoint(monet_generator=monet_generator,\n                           photo_generator=photo_generator,\n                           monet_discriminator=monet_discriminator,\n                           photo_discriminator=photo_discriminator,\n                           monet_generator_optimizer=monet_generator_optimizer,\n                           photo_generator_optimizer=photo_generator_optimizer,\n                           monet_discriminator_optimizer=monet_discriminator_optimizer,\n                           photo_discriminator_optimizer=photo_discriminator_optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, CHECKPOINT_DIR, max_to_keep=5)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print ('Latest checkpoint restored!!')","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:50:21.401922Z","iopub.execute_input":"2024-10-01T23:50:21.402467Z","iopub.status.idle":"2024-10-01T23:50:21.412966Z","shell.execute_reply.started":"2024-10-01T23:50:21.402397Z","shell.execute_reply":"2024-10-01T23:50:21.411385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create the training step for the definition of the loop later\n@tf.function\ndef train_step(real_monet, real_photo):\n    with tf.GradientTape(persistent=True) as tape:\n        # real monet -> fake photo\n        fake_photo = monet_generator(real_monet, training=True)\n        # fake photo -> monet\n        cycled_monet = photo_generator(fake_photo, training=True)\n        # real photo -> fake monet\n        fake_monet = photo_generator(real_photo, training=True)\n        # fake monet -> photo\n        cycled_photo = monet_generator(fake_monet, training=True)\n        # use for identity loss\n        same_monet = monet_generator(real_monet, training=True)\n        same_photo = photo_generator(real_photo, training=True)\n        \n        # The discriminators evaluate both real and fake images in each domain\n        disc_real_monet = monet_discriminator(real_monet, training=True)\n        disc_real_photo = photo_discriminator(real_photo, training=True)\n        disc_fake_monet = monet_discriminator(fake_monet, training=True)\n        disc_fake_photo = photo_discriminator(fake_photo, training=True)\n        \n        # Generator losses: How well the generators fool the discriminators\n        monet_gen_loss = generator_loss(disc_fake_photo)\n        photo_gen_loss = generator_loss(disc_fake_monet)\n        # Cycle consistency loss: Ensures that cycled images are similar to originals\n        total_cycle_loss = compute_cycle_loss(real_monet, cycled_monet) + compute_cycle_loss(real_photo, cycled_photo)\n        # Identity loss: Encourages generators to preserve input when it's already in the target domain\n        total_monet_gen_loss = monet_gen_loss + total_cycle_loss + identity_loss(real_monet, same_monet)\n        total_photo_gen_loss = photo_gen_loss + total_cycle_loss + identity_loss(real_photo, same_photo)\n        # Discriminator losses: How well the discriminators distinguish real from fake images\n        monet_disc_loss = discriminator_loss(disc_real_monet, disc_fake_monet)\n        photo_disc_loss = discriminator_loss(disc_real_photo, disc_fake_photo)\n    \n    # Computes the gradients for each model with respect to its loss.\n    monet_generator_gradients = tape.gradient(total_monet_gen_loss, monet_generator.trainable_variables)\n    photo_generator_gradients = tape.gradient(total_photo_gen_loss, photo_generator.trainable_variables)\n    monet_discriminator_gradients = tape.gradient(monet_disc_loss, monet_discriminator.trainable_variables)\n    photo_discriminator_gradients = tape.gradient(photo_disc_loss, photo_discriminator.trainable_variables)\n    \n    # Updates the model parameters using the calculated gradients.\n    monet_generator_optimizer.apply_gradients(zip(monet_generator_gradients, monet_generator.trainable_variables))\n    photo_generator_optimizer.apply_gradients(zip(photo_generator_gradients, photo_generator.trainable_variables))\n    monet_discriminator_optimizer.apply_gradients(zip(monet_discriminator_gradients, monet_discriminator.trainable_variables))\n    photo_discriminator_optimizer.apply_gradients(zip(photo_discriminator_gradients, photo_discriminator.trainable_variables))","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:50:21.414959Z","iopub.execute_input":"2024-10-01T23:50:21.415473Z","iopub.status.idle":"2024-10-01T23:50:21.432032Z","shell.execute_reply.started":"2024-10-01T23:50:21.415422Z","shell.execute_reply":"2024-10-01T23:50:21.430613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output\n\ndef train(monet_dataset, photo_dataset, epochs):\n    n = 0\n    for epoch in range(epochs):\n        start = time.time()\n        for monet_image, photo_image in tf.data.Dataset.zip((monet_dataset, photo_dataset)):\n            train_step(monet_image, photo_image)\n            if n % 10 == 0:\n                print ('.', end='')\n            n += 1\n        clear_output(wait=True)\n        if (epoch + 1) % 5 == 0:\n            ckpt_save_path = ckpt_manager.save()\n            print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))\n        \n        print(f'Time taken for epoch {epoch + 1} is {time.time() - start} sec')","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:50:21.433668Z","iopub.execute_input":"2024-10-01T23:50:21.434199Z","iopub.status.idle":"2024-10-01T23:50:21.449917Z","shell.execute_reply.started":"2024-10-01T23:50:21.434140Z","shell.execute_reply":"2024-10-01T23:50:21.448671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_images(model, test_input):\n    prediction = model(test_input)\n    plt.figure(figsize=(12, 12))\n    display_list = [test_input[0], prediction[0]]\n    title = ['Input Image', 'Monet-style Image']\n    for i in range(2):\n        plt.subplot(1, 2, i+1)\n        plt.title(title[i])\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis('off')\n    plt.show()\n\n# Train the model\ntrain(train_ds, test_ds, epochs=EPOCHS)\n\n#Show some sample output\nfor photo in test_ds.take(5):\n    generate_images(monet_generator, photo)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:50:21.451473Z","iopub.execute_input":"2024-10-01T23:50:21.451986Z","iopub.status.idle":"2024-10-02T00:16:25.459776Z","shell.execute_reply.started":"2024-10-01T23:50:21.451927Z","shell.execute_reply":"2024-10-02T00:16:25.458006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! mkdir ../images","metadata":{"execution":{"iopub.status.busy":"2024-10-02T00:16:25.461121Z","iopub.status.idle":"2024-10-02T00:16:25.461649Z","shell.execute_reply.started":"2024-10-02T00:16:25.461400Z","shell.execute_reply":"2024-10-02T00:16:25.461424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\ni = 1\nfor photo in test_ds:\n    prediction = monet_generator(photo, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    im = PIL.Image.fromarray(prediction)\n    im.save(\"../images/\" + str(i) + \".jpg\")\n    i += 1\n    \nshutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")","metadata":{},"execution_count":null,"outputs":[]}]}